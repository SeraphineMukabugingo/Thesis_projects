{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8efef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: (14634, 66)\n",
      "\n",
      "Starting Phase 3.1: Enhanced Data Preparation...\n",
      "================================================================================\n",
      "PHASE 3.1: ENHANCED DATA PREPARATION WITH FEATURE ENGINEERING\n",
      "================================================================================\n",
      "Dataset after removing missing targets: (14634, 66)\n",
      "\n",
      "COLUMNS EXCLUDED FROM FEATURES:\n",
      "  ID variables: ['caseid', 'household_id', 'v001', 'v002']\n",
      "  Leakage variables: ['v525', 'v512', 'v511', 'v212']\n",
      "  Target variable: ['early_sexual_debut']\n",
      "  Total excluded: 9\n",
      "✓ Confirmed: All problematic variables excluded from features\n",
      "✓ Total features available: 61\n",
      "\n",
      "TARGET VARIABLE DISTRIBUTION:\n",
      "  Late debut (0): 7,919 (54.1%)\n",
      "  Early debut (1): 6,715 (45.9%)\n",
      "\n",
      "DATA SPLIT SUMMARY:\n",
      "Train: 10243, Val: 2195, Test: 2196\n",
      "\n",
      "Starting Phase 3.2: Precision-Optimized Model Implementation...\n",
      "\n",
      "================================================================================\n",
      "PHASE 3.2: PRECISION-OPTIMIZED MODEL IMPLEMENTATION\n",
      "================================================================================\n",
      "\n",
      "ADVANCED FEATURE ENGINEERING\n",
      "--------------------------------------------------\n",
      "Step 1: Removing highly correlated features...\n",
      "  Removing 22 highly correlated features\n",
      "Step 2: Advanced feature selection...\n",
      "  Selected top 25 features\n",
      "  Top 10 features: ['v012', 'v201', 'ever_married', 'b5', 'age_education_interaction', 'v191', 'v152', 'v150', 'hv009', 'v312']\n",
      "Step 3: Creating interaction features...\n",
      "  Final feature count: 35\n",
      "  Added 10 interaction features\n",
      "\n",
      "PRECISION-FOCUSED SAMPLING: CONSERVATIVE_OVERSAMPLE\n",
      "--------------------------------------------------\n",
      "Original distribution: {0: np.int64(5543), 1: np.int64(4700)}\n",
      "Resampled distribution: {0: np.int64(5543), 1: np.int64(4700)}\n",
      "✓ Precision-focused sampling completed\n",
      "Conservative class weights: {0: 1.0, 1: np.float64(1.7690425531914895)}\n",
      "\n",
      "3.2.1 PRECISION-OPTIMIZED LOGISTIC REGRESSION\n",
      "-------------------------------------------------------\n",
      "Tuned LR - Recall: 0.7676, Precision: 0.8551, F1: 0.8090\n",
      "Best params: {'C': 0.5, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "\n",
      "3.2.2 PRECISION-OPTIMIZED RANDOM FOREST\n",
      "--------------------------------------------------\n",
      "Conservative RF - Recall: 0.8510, Precision: 0.7504, F1: 0.7976\n",
      "OOB Score: 0.8239\n",
      "\n",
      "3.2.3 PRECISION-OPTIMIZED XGBOOST\n",
      "---------------------------------------------\n",
      "Regularized XGB - Recall: 0.8759, Precision: 0.7206, F1: 0.7907\n",
      "\n",
      "3.2.4 STACKING ENSEMBLE FOR PRECISION OPTIMIZATION\n",
      "------------------------------------------------------------\n",
      "Stacking Ensemble - Recall: 0.8491, Precision: 0.7553, F1: 0.7994\n",
      "\n",
      "Starting Phase 3.3: Tiered Intervention Strategy...\n",
      "\n",
      "================================================================================\n",
      "TIERED INTERVENTION THRESHOLD OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "Optimizing Logistic Regression...\n",
      "  High Risk Tier (≥0.7): Precision=0.9256, Recall=0.6177, Coverage=30.62%\n",
      "  Medium Risk Tier (≥0.4): Precision=0.8283, Recall=0.7905, Coverage=43.78%\n",
      "  Low Risk Tier (≥0.2): Precision=0.6435, Recall=0.9176, Coverage=65.42%\n",
      "\n",
      "Optimizing Random Forest...\n",
      "  High Risk Tier (≥0.7): Precision=0.9209, Recall=0.6822, Coverage=33.99%\n",
      "  Medium Risk Tier (≥0.4): Precision=0.7504, Recall=0.8510, Coverage=52.03%\n",
      "  Low Risk Tier (≥0.2): Precision=0.5675, Recall=0.9722, Coverage=78.59%\n",
      "\n",
      "Optimizing Xgboost...\n",
      "  High Risk Tier (≥0.7): Precision=0.8783, Recall=0.7378, Coverage=38.54%\n",
      "  Medium Risk Tier (≥0.4): Precision=0.7308, Recall=0.8679, Coverage=54.49%\n",
      "  Low Risk Tier (≥0.2): Precision=0.5980, Recall=0.9543, Coverage=73.21%\n",
      "\n",
      "Optimizing Stacking Ensemble...\n",
      "  High Risk Tier (≥0.7): Precision=0.8910, Recall=0.7468, Coverage=38.45%\n",
      "  Medium Risk Tier (≥0.4): Precision=0.7386, Recall=0.8560, Coverage=53.17%\n",
      "  Low Risk Tier (≥0.2): Precision=0.5877, Recall=0.9583, Coverage=74.81%\n",
      "\n",
      "================================================================================\n",
      "TIERED INTERVENTION RESULTS:\n",
      "================================================================================\n",
      "              Model        Tier  Threshold  Recall  Precision     F1  Coverage    AUC\n",
      "Logistic Regression   High Risk        0.7  0.6177     0.9256 0.7409    0.3062 0.8958\n",
      "Logistic Regression Medium Risk        0.4  0.7905     0.8283 0.8089    0.4378 0.8958\n",
      "Logistic Regression    Low Risk        0.2  0.9176     0.6435 0.7564    0.6542 0.8958\n",
      "      Random Forest   High Risk        0.7  0.6822     0.9209 0.7838    0.3399 0.9010\n",
      "      Random Forest Medium Risk        0.4  0.8510     0.7504 0.7976    0.5203 0.9010\n",
      "      Random Forest    Low Risk        0.2  0.9722     0.5675 0.7167    0.7859 0.9010\n",
      "            Xgboost   High Risk        0.7  0.7378     0.8783 0.8019    0.3854 0.9010\n",
      "            Xgboost Medium Risk        0.4  0.8679     0.7308 0.7935    0.5449 0.9010\n",
      "            Xgboost    Low Risk        0.2  0.9543     0.5980 0.7353    0.7321 0.9010\n",
      "  Stacking Ensemble   High Risk        0.7  0.7468     0.8910 0.8125    0.3845 0.9028\n",
      "  Stacking Ensemble Medium Risk        0.4  0.8560     0.7386 0.7930    0.5317 0.9028\n",
      "  Stacking Ensemble    Low Risk        0.2  0.9583     0.5877 0.7286    0.7481 0.9028\n",
      "\n",
      "============================================================\n",
      "TIERED STRATEGY SUMMARY:\n",
      "============================================================\n",
      "High-precision tiers (≥70% precision): 4\n",
      "High-recall tiers (≥90% recall): 4\n",
      "\n",
      "Best high-precision tier: Logistic Regression - High Risk\n",
      "  Precision: 0.9256\n",
      "  Recall: 0.6177\n",
      "  Population Coverage: 30.62%\n",
      "\n",
      "Best high-recall tier: Random Forest - Low Risk\n",
      "  Recall: 0.9722\n",
      "  Precision: 0.5675\n",
      "  Population Coverage: 78.59%\n",
      "\n",
      "Starting Phase 3.4: Comprehensive Evaluation...\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION - REALISTIC TARGETS\n",
      "================================================================================\n",
      "STANDARD MODEL PERFORMANCE:\n",
      "=====================================================================================\n",
      "              Model    AUC  Precision  Recall     F1  Accuracy  Threshold\n",
      "Logistic Regression 0.8958     0.8551  0.7676 0.8090    0.8337       0.45\n",
      "      Random Forest 0.9010     0.7504  0.8510 0.7976    0.8018       0.40\n",
      "            Xgboost 0.9010     0.7206  0.8759 0.7907    0.7872       0.38\n",
      "  Stacking Ensemble 0.9028     0.7553  0.8491 0.7994    0.8046       0.42\n",
      "\n",
      "BEST INDIVIDUAL PERFORMANCES:\n",
      "Highest Precision: Logistic Regression - 0.8551\n",
      "Highest Recall: Xgboost - 0.8759\n",
      "Highest F1: Logistic Regression - 0.8090\n",
      "\n",
      "REALISTIC PERFORMANCE ANALYSIS:\n",
      "Models with ≥85% recall: 2/4\n",
      "Models with ≥65% precision: 4/4\n",
      "Models achieving BOTH (≥85% recall & ≥65% precision): 2/4\n",
      "\n",
      "ACHIEVABLE BALANCED MODELS:\n",
      "  Random Forest: Recall=0.8510, Precision=0.7504, F1=0.7976\n",
      "  Xgboost: Recall=0.8759, Precision=0.7206, F1=0.7907\n",
      "\n",
      "================================================================================\n",
      "PRECISION-RECALL OPTIMIZATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "OPTIMAL ACHIEVABLE PERFORMANCE:\n",
      "Model: Random Forest\n",
      "  Recall: 0.8510 (85.1%)\n",
      "  Precision: 0.7504 (75.0%)\n",
      "  F1 Score: 0.7976\n",
      "  AUC: 0.9010\n",
      "  Cases missed: 14.9%\n",
      "  False positive rate: 25.0%\n",
      "\n",
      "TIERED STRATEGY RECOMMENDATION:\n",
      "High-Precision Tier: Logistic Regression at threshold 0.7\n",
      "  Precision: 0.9256\n",
      "  Recall: 0.6177\n",
      "  Population Coverage: 30.62%\n",
      "  Intervention: Intensive individual support\n",
      "\n",
      "REALISTIC TARGET ACHIEVEMENT:\n",
      "  Models achieving ≥85% recall: 2/4\n",
      "  Models achieving ≥68% precision: 4/4\n",
      "\n",
      "DEPLOYMENT READINESS:\n",
      "  Advanced feature engineering completed\n",
      "  Conservative sampling strategy applied\n",
      "  Tiered intervention thresholds optimized\n",
      "  Realistic performance targets identified\n",
      "  Ready for pilot implementation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from itertools import combinations\n",
    "\n",
    "# Algorithm imports\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_enhanced_data(df, target_col='early_sexual_debut', test_size=0.15, val_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Enhanced data preparation with advanced feature engineering\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PHASE 3.1: ENHANCED DATA PREPARATION WITH FEATURE ENGINEERING\")  \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Remove rows with missing target\n",
    "    df_clean = df.dropna(subset=[target_col]).copy()\n",
    "    print(f\"Dataset after removing missing targets: {df_clean.shape}\")\n",
    "    \n",
    "    # CRITICAL: Define all columns to exclude from features\n",
    "    id_columns = ['caseid', 'household_id', 'v001', 'v002']\n",
    "    leakage_variables = ['v525', 'v512', 'v511', 'v212']\n",
    "    target_variables = [target_col]\n",
    "    exclude_columns = id_columns + leakage_variables + target_variables\n",
    "    \n",
    "    print(f\"\\nCOLUMNS EXCLUDED FROM FEATURES:\")\n",
    "    print(f\"  ID variables: {id_columns}\")\n",
    "    print(f\"  Leakage variables: {leakage_variables}\")\n",
    "    print(f\"  Target variable: {target_variables}\")\n",
    "    print(f\"  Total excluded: {len(exclude_columns)}\")\n",
    "    \n",
    "    # Create feature column list\n",
    "    feature_columns = [col for col in df_clean.columns \n",
    "                      if col not in exclude_columns]\n",
    "    \n",
    "    # VERIFICATION: Ensure target and leakage variables are not in features\n",
    "    problematic_vars = [var for var in exclude_columns if var in feature_columns]\n",
    "    if problematic_vars:\n",
    "        raise ValueError(f\"CRITICAL ERROR: Problematic variables found in features: {problematic_vars}\")\n",
    "    \n",
    "    print(f\"✓ Confirmed: All problematic variables excluded from features\")\n",
    "    print(f\"✓ Total features available: {len(feature_columns)}\")\n",
    "    \n",
    "    # Prepare features (X) and target (y) separately\n",
    "    X = df_clean[feature_columns].copy()\n",
    "    y = df_clean[target_col].copy()\n",
    "    \n",
    "    # Enhanced missing value handling with KNN imputation\n",
    "    missing_features = X.isnull().sum()\n",
    "    if missing_features.sum() > 0:\n",
    "        print(f\"\\nHandling missing values in {(missing_features > 0).sum()} features...\")\n",
    "        from sklearn.impute import KNNImputer\n",
    "        \n",
    "        knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "        X_numeric = X.select_dtypes(include=[np.number])\n",
    "        X_categorical = X.select_dtypes(exclude=[np.number])\n",
    "        \n",
    "        if len(X_numeric.columns) > 0:\n",
    "            X_numeric_imputed = pd.DataFrame(\n",
    "                knn_imputer.fit_transform(X_numeric),\n",
    "                columns=X_numeric.columns,\n",
    "                index=X_numeric.index\n",
    "            )\n",
    "            X[X_numeric.columns] = X_numeric_imputed\n",
    "        \n",
    "        if len(X_categorical.columns) > 0:\n",
    "            from sklearn.impute import SimpleImputer\n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            X[X_categorical.columns] = cat_imputer.fit_transform(X[X_categorical.columns])\n",
    "        \n",
    "        print(f\"✓ Advanced missing value imputation completed\")\n",
    "    \n",
    "    # Convert target to integer\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    # Enhanced class distribution analysis\n",
    "    class_counts = pd.Series(y).value_counts().sort_index()\n",
    "    class_props = class_counts / len(y)\n",
    "    \n",
    "    print(f\"\\nTARGET VARIABLE DISTRIBUTION:\")\n",
    "    for class_val, count in class_counts.items():\n",
    "        prop = class_props[class_val]\n",
    "        label = \"Early debut\" if class_val == 1 else \"Late debut\"\n",
    "        print(f\"  {label} ({class_val}): {count:,} ({prop:.1%})\")\n",
    "    \n",
    "    # Stratified data splitting\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    adjusted_val_size = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=adjusted_val_size,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDATA SPLIT SUMMARY:\")\n",
    "    print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val, \n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'feature_columns': feature_columns,\n",
    "        'class_distribution': class_counts,\n",
    "        'excluded_columns': exclude_columns\n",
    "    }\n",
    "\n",
    "def advanced_feature_engineering(X_train, y_train, X_val, X_test=None):\n",
    "    \"\"\"\n",
    "    Advanced feature engineering for better precision-recall balance\n",
    "    \"\"\"\n",
    "    print(f\"\\nADVANCED FEATURE ENGINEERING\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Step 1: Remove highly correlated features\n",
    "    print(\"Step 1: Removing highly correlated features...\")\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = numeric_features.corr().abs()\n",
    "    \n",
    "    # Find features with correlation > 0.85\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if correlation_matrix.iloc[i, j] > 0.85:\n",
    "                high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n",
    "    \n",
    "    # Remove one feature from each highly correlated pair\n",
    "    features_to_remove = set()\n",
    "    for feat1, feat2 in high_corr_pairs:\n",
    "        # Keep the feature with higher correlation to target\n",
    "        corr1 = abs(np.corrcoef(X_train[feat1].fillna(X_train[feat1].median()), y_train)[0, 1])\n",
    "        corr2 = abs(np.corrcoef(X_train[feat2].fillna(X_train[feat2].median()), y_train)[0, 1])\n",
    "        \n",
    "        if corr1 >= corr2:\n",
    "            features_to_remove.add(feat2)\n",
    "        else:\n",
    "            features_to_remove.add(feat1)\n",
    "    \n",
    "    print(f\"  Removing {len(features_to_remove)} highly correlated features\")\n",
    "    \n",
    "    # Step 2: Feature selection using multiple methods\n",
    "    print(\"Step 2: Advanced feature selection...\")\n",
    "    \n",
    "    # Remove highly correlated features\n",
    "    remaining_features = [col for col in X_train.columns if col not in features_to_remove]\n",
    "    X_train_filtered = X_train[remaining_features]\n",
    "    X_val_filtered = X_val[remaining_features]\n",
    "    X_test_filtered = X_test[remaining_features] if X_test is not None else None\n",
    "    \n",
    "    # Use Random Forest for feature importance\n",
    "    rf_selector = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf_selector.fit(X_train_filtered, y_train)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train_filtered.columns,\n",
    "        'importance': rf_selector.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Select top 25 features for better precision\n",
    "    top_features = feature_importance.head(25)['feature'].tolist()\n",
    "    print(f\"  Selected top {len(top_features)} features\")\n",
    "    print(f\"  Top 10 features: {top_features[:10]}\")\n",
    "    \n",
    "    # Step 3: Create interaction features for top predictors\n",
    "    print(\"Step 3: Creating interaction features...\")\n",
    "    \n",
    "    # Select top 8 features for interactions to avoid explosion\n",
    "    interaction_features = top_features[:8]\n",
    "    X_train_selected = X_train_filtered[top_features]\n",
    "    X_val_selected = X_val_filtered[top_features]\n",
    "    X_test_selected = X_test_filtered[top_features] if X_test_filtered is not None else None\n",
    "    \n",
    "    # Create polynomial features for top 5 most important features\n",
    "    top_5_features = top_features[:5]\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    \n",
    "    X_train_poly = poly.fit_transform(X_train_filtered[top_5_features])\n",
    "    X_val_poly = poly.transform(X_val_filtered[top_5_features])\n",
    "    X_test_poly = poly.transform(X_test_filtered[top_5_features]) if X_test_filtered is not None else None\n",
    "    \n",
    "    # Combine original features with polynomial features\n",
    "    poly_feature_names = poly.get_feature_names_out(top_5_features)\n",
    "    \n",
    "    # Convert to DataFrame and combine\n",
    "    X_train_poly_df = pd.DataFrame(X_train_poly, columns=poly_feature_names, index=X_train_selected.index)\n",
    "    X_val_poly_df = pd.DataFrame(X_val_poly, columns=poly_feature_names, index=X_val_selected.index)\n",
    "    \n",
    "    if X_test_selected is not None:\n",
    "        X_test_poly_df = pd.DataFrame(X_test_poly, columns=poly_feature_names, index=X_test_selected.index)\n",
    "    \n",
    "    # Select non-interaction terms from polynomial features (avoid original duplicates)\n",
    "    original_feature_names = set(top_5_features)\n",
    "    poly_only_features = [col for col in poly_feature_names if col not in original_feature_names]\n",
    "    \n",
    "    # Final feature combination: top 25 + polynomial interactions\n",
    "    X_train_final = pd.concat([\n",
    "        X_train_selected,\n",
    "        X_train_poly_df[poly_only_features]\n",
    "    ], axis=1)\n",
    "    \n",
    "    X_val_final = pd.concat([\n",
    "        X_val_selected,\n",
    "        X_val_poly_df[poly_only_features]\n",
    "    ], axis=1)\n",
    "    \n",
    "    if X_test_selected is not None:\n",
    "        X_test_final = pd.concat([\n",
    "            X_test_selected,\n",
    "            X_test_poly_df[poly_only_features]\n",
    "        ], axis=1)\n",
    "    else:\n",
    "        X_test_final = None\n",
    "    \n",
    "    print(f\"  Final feature count: {X_train_final.shape[1]}\")\n",
    "    print(f\"  Added {len(poly_only_features)} interaction features\")\n",
    "    \n",
    "    final_features = X_train_final.columns.tolist()\n",
    "    \n",
    "    return X_train_final, X_val_final, X_test_final, final_features\n",
    "\n",
    "def precision_focused_sampling(X_train, y_train, strategy='conservative_oversample', random_state=42):\n",
    "    \"\"\"\n",
    "    Precision-focused sampling strategy\n",
    "    \"\"\"\n",
    "    print(f\"\\nPRECISION-FOCUSED SAMPLING: {strategy.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    original_counts = pd.Series(y_train).value_counts()\n",
    "    print(f\"Original distribution: {dict(original_counts)}\")\n",
    "    \n",
    "    if strategy == 'conservative_oversample':\n",
    "        # Conservative oversampling: 1.3:1 ratio instead of 1:1\n",
    "        minority_mask = y_train == 1\n",
    "        majority_mask = y_train == 0\n",
    "        \n",
    "        minority_X = X_train[minority_mask]\n",
    "        minority_y = y_train[minority_mask]\n",
    "        majority_X = X_train[majority_mask]\n",
    "        majority_y = y_train[majority_mask]\n",
    "        \n",
    "        n_minority = len(minority_y)\n",
    "        n_majority = len(majority_y)\n",
    "        target_minority = int(n_majority / 1.3)  # 1.3:1 ratio\n",
    "        \n",
    "        if target_minority > n_minority:\n",
    "            np.random.seed(random_state)\n",
    "            bootstrap_indices = np.random.choice(n_minority, \n",
    "                                               size=target_minority - n_minority, \n",
    "                                               replace=True)\n",
    "            \n",
    "            additional_X = minority_X.iloc[bootstrap_indices]\n",
    "            additional_y = minority_y.iloc[bootstrap_indices]\n",
    "            \n",
    "            X_resampled = pd.concat([majority_X, minority_X, additional_X]).reset_index(drop=True)\n",
    "            y_resampled = pd.concat([majority_y, minority_y, additional_y]).reset_index(drop=True)\n",
    "        else:\n",
    "            X_resampled = pd.concat([majority_X, minority_X]).reset_index(drop=True)\n",
    "            y_resampled = pd.concat([majority_y, minority_y]).reset_index(drop=True)\n",
    "            \n",
    "    elif strategy == 'stratified_oversample':\n",
    "        # Stratified oversampling based on feature importance\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        # Cluster minority class samples\n",
    "        minority_mask = y_train == 1\n",
    "        minority_X = X_train[minority_mask]\n",
    "        minority_y = y_train[minority_mask]\n",
    "        \n",
    "        # Create clusters within minority class\n",
    "        kmeans = KMeans(n_clusters=5, random_state=random_state)\n",
    "        clusters = kmeans.fit_predict(minority_X)\n",
    "        \n",
    "        # Oversample each cluster proportionally\n",
    "        oversampled_X_list = []\n",
    "        oversampled_y_list = []\n",
    "        \n",
    "        for cluster_id in np.unique(clusters):\n",
    "            cluster_mask = clusters == cluster_id\n",
    "            cluster_X = minority_X[cluster_mask]\n",
    "            cluster_y = minority_y[cluster_mask]\n",
    "            \n",
    "            # Oversample this cluster\n",
    "            n_samples = len(cluster_X)\n",
    "            additional_samples = max(1, int(n_samples * 0.3))  # 30% additional\n",
    "            \n",
    "            if additional_samples > 0:\n",
    "                bootstrap_indices = np.random.choice(n_samples, \n",
    "                                                   size=additional_samples, \n",
    "                                                   replace=True)\n",
    "                \n",
    "                additional_X = cluster_X.iloc[bootstrap_indices]\n",
    "                additional_y = cluster_y.iloc[bootstrap_indices]\n",
    "                \n",
    "                oversampled_X_list.append(pd.concat([cluster_X, additional_X]))\n",
    "                oversampled_y_list.append(pd.concat([cluster_y, additional_y]))\n",
    "            else:\n",
    "                oversampled_X_list.append(cluster_X)\n",
    "                oversampled_y_list.append(cluster_y)\n",
    "        \n",
    "        # Combine all oversampled clusters with majority class\n",
    "        majority_X = X_train[y_train == 0]\n",
    "        majority_y = y_train[y_train == 0]\n",
    "        \n",
    "        minority_oversampled_X = pd.concat(oversampled_X_list).reset_index(drop=True)\n",
    "        minority_oversampled_y = pd.concat(oversampled_y_list).reset_index(drop=True)\n",
    "        \n",
    "        X_resampled = pd.concat([majority_X, minority_oversampled_X]).reset_index(drop=True)\n",
    "        y_resampled = pd.concat([majority_y, minority_oversampled_y]).reset_index(drop=True)\n",
    "        \n",
    "    else:\n",
    "        print(\"No sampling applied\")\n",
    "        return X_train, y_train\n",
    "    \n",
    "    resampled_counts = pd.Series(y_resampled).value_counts()\n",
    "    print(f\"Resampled distribution: {dict(resampled_counts)}\")\n",
    "    print(f\"✓ Precision-focused sampling completed\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def implement_precision_optimized_models(data_splits, random_state=42):\n",
    "    \"\"\"\n",
    "    Implement precision-optimized models with advanced techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 3.2: PRECISION-OPTIMIZED MODEL IMPLEMENTATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train = data_splits['X_train']\n",
    "    y_train = data_splits['y_train']\n",
    "    X_val = data_splits['X_val'] \n",
    "    y_val = data_splits['y_val']\n",
    "    X_test = data_splits.get('X_test', None)\n",
    "    \n",
    "    # Advanced feature engineering\n",
    "    X_train_eng, X_val_eng, X_test_eng, engineered_features = advanced_feature_engineering(\n",
    "        X_train, y_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Precision-focused sampling\n",
    "    X_train_balanced, y_train_balanced = precision_focused_sampling(\n",
    "        X_train_eng, y_train, strategy='conservative_oversample', random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Calculate conservative class weights\n",
    "    class_counts = pd.Series(y_train_balanced).value_counts().sort_index()\n",
    "    class_weights = {\n",
    "        0: 1.0,\n",
    "        1: (class_counts[0] / class_counts[1]) * 1.5  # Conservative 1.5x multiplier\n",
    "    }\n",
    "    \n",
    "    print(f\"Conservative class weights: {class_weights}\")\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # ================================================================\n",
    "    # PRECISION-OPTIMIZED LOGISTIC REGRESSION WITH HYPERPARAMETER TUNING\n",
    "    # ================================================================\n",
    "    \n",
    "    print(f\"\\n3.2.1 PRECISION-OPTIMIZED LOGISTIC REGRESSION\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Hyperparameter tuning for Logistic Regression\n",
    "    scaler_lr = RobustScaler()\n",
    "    X_train_scaled = scaler_lr.fit_transform(X_train_balanced)\n",
    "    X_val_scaled = scaler_lr.transform(X_val_eng)\n",
    "    \n",
    "    # Grid search for best parameters\n",
    "    lr_params = {\n",
    "        'C': [0.5, 0.8, 1.0, 1.2, 1.5],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    }\n",
    "    \n",
    "    lr_base = LogisticRegression(\n",
    "        class_weight=class_weights,\n",
    "        random_state=random_state,\n",
    "        max_iter=3000\n",
    "    )\n",
    "    \n",
    "    lr_grid = GridSearchCV(lr_base, lr_params, cv=3, scoring='f1', n_jobs=-1)\n",
    "    lr_grid.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    # Get best model and calibrate\n",
    "    best_lr = lr_grid.best_estimator_\n",
    "    lr_model = CalibratedClassifierCV(best_lr, method='isotonic', cv=3)\n",
    "    lr_model.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    lr_val_proba = lr_model.predict_proba(X_val_scaled)[:, 1]\n",
    "    lr_threshold = 0.45  # Conservative threshold for precision\n",
    "    lr_val_pred = (lr_val_proba >= lr_threshold).astype(int)\n",
    "    \n",
    "    models['logistic_regression'] = {\n",
    "        'model': lr_model,\n",
    "        'scaler': scaler_lr,\n",
    "        'val_predictions': lr_val_pred,\n",
    "        'val_probabilities': lr_val_proba,\n",
    "        'threshold': lr_threshold,\n",
    "        'features': engineered_features,\n",
    "        'requires_scaling': True\n",
    "    }\n",
    "    \n",
    "    lr_metrics = {\n",
    "        'recall': recall_score(y_val, lr_val_pred),\n",
    "        'precision': precision_score(y_val, lr_val_pred),\n",
    "        'f1': f1_score(y_val, lr_val_pred)\n",
    "    }\n",
    "    print(f\"Tuned LR - Recall: {lr_metrics['recall']:.4f}, Precision: {lr_metrics['precision']:.4f}, F1: {lr_metrics['f1']:.4f}\")\n",
    "    print(f\"Best params: {lr_grid.best_params_}\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # PRECISION-OPTIMIZED RANDOM FOREST WITH CONSERVATIVE PARAMETERS\n",
    "    # ================================================================\n",
    "    \n",
    "    print(f\"\\n3.2.2 PRECISION-OPTIMIZED RANDOM FOREST\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Conservative Random Forest parameters for better precision\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=600,       # More trees for stability\n",
    "        max_depth=15,           # Reduced depth to prevent overfitting\n",
    "        min_samples_split=25,   # Higher minimum samples for splits\n",
    "        min_samples_leaf=10,    # Larger leaf size for generalization\n",
    "        max_features='log2',    # Fewer features per split\n",
    "        class_weight=class_weights,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        criterion='gini',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        max_samples=0.7         # Conservative bootstrap sampling\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "    rf_val_proba = rf_model.predict_proba(X_val_eng)[:, 1]\n",
    "    \n",
    "    rf_threshold = 0.40  # Higher threshold for precision\n",
    "    rf_val_pred = (rf_val_proba >= rf_threshold).astype(int)\n",
    "    \n",
    "    models['random_forest'] = {\n",
    "        'model': rf_model,\n",
    "        'scaler': None,\n",
    "        'val_predictions': rf_val_pred,\n",
    "        'val_probabilities': rf_val_proba,\n",
    "        'threshold': rf_threshold,\n",
    "        'features': engineered_features,\n",
    "        'requires_scaling': False\n",
    "    }\n",
    "    \n",
    "    rf_metrics = {\n",
    "        'recall': recall_score(y_val, rf_val_pred),\n",
    "        'precision': precision_score(y_val, rf_val_pred),\n",
    "        'f1': f1_score(y_val, rf_val_pred)\n",
    "    }\n",
    "    print(f\"Conservative RF - Recall: {rf_metrics['recall']:.4f}, Precision: {rf_metrics['precision']:.4f}, F1: {rf_metrics['f1']:.4f}\")\n",
    "    print(f\"OOB Score: {rf_model.oob_score_:.4f}\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # PRECISION-OPTIMIZED XGBOOST WITH REGULARIZATION\n",
    "    # ================================================================\n",
    "    \n",
    "    print(f\"\\n3.2.3 PRECISION-OPTIMIZED XGBOOST\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Conservative XGBoost with heavy regularization\n",
    "    scale_pos_weight = (class_counts[0] / class_counts[1]) * 1.8  # Reduced scaling\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.06,     # Lower learning rate\n",
    "        max_depth=4,            # Shallow trees\n",
    "        min_child_weight=6,     # Higher minimum child weight\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=random_state,\n",
    "        eval_metric='aucpr',\n",
    "        reg_alpha=0.4,          # Heavy L1 regularization\n",
    "        reg_lambda=0.4,         # Heavy L2 regularization\n",
    "        tree_method='hist',\n",
    "        objective='binary:logistic',\n",
    "        gamma=0.3               # Higher minimum split loss\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train_balanced, y_train_balanced,\n",
    "                  eval_set=[(X_val_eng, y_val)],\n",
    "                  verbose=False)\n",
    "    \n",
    "    xgb_val_proba = xgb_model.predict_proba(X_val_eng)[:, 1]\n",
    "    xgb_threshold = 0.38  # Conservative threshold\n",
    "    xgb_val_pred = (xgb_val_proba >= xgb_threshold).astype(int)\n",
    "    \n",
    "    models['xgboost'] = {\n",
    "        'model': xgb_model,\n",
    "        'scaler': None,\n",
    "        'val_predictions': xgb_val_pred,\n",
    "        'val_probabilities': xgb_val_proba,\n",
    "        'threshold': xgb_threshold,\n",
    "        'features': engineered_features,\n",
    "        'requires_scaling': False\n",
    "    }\n",
    "    \n",
    "    xgb_metrics = {\n",
    "        'recall': recall_score(y_val, xgb_val_pred),\n",
    "        'precision': precision_score(y_val, xgb_val_pred),\n",
    "        'f1': f1_score(y_val, xgb_val_pred)\n",
    "    }\n",
    "    print(f\"Regularized XGB - Recall: {xgb_metrics['recall']:.4f}, Precision: {xgb_metrics['precision']:.4f}, F1: {xgb_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # STACKING ENSEMBLE FOR MAXIMUM PRECISION\n",
    "    # ================================================================\n",
    "    \n",
    "    print(f\"\\n3.2.4 STACKING ENSEMBLE FOR PRECISION OPTIMIZATION\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create base models with conservative parameters\n",
    "    base_models = [\n",
    "        ('rf_conservative', RandomForestClassifier(\n",
    "            n_estimators=400, max_depth=12, min_samples_split=30,\n",
    "            min_samples_leaf=15, class_weight=class_weights, \n",
    "            random_state=random_state, n_jobs=-1\n",
    "        )),\n",
    "        ('xgb_conservative', xgb.XGBClassifier(\n",
    "            n_estimators=400, learning_rate=0.08, max_depth=3,\n",
    "            min_child_weight=8, scale_pos_weight=scale_pos_weight,\n",
    "            reg_alpha=0.5, reg_lambda=0.5, random_state=random_state\n",
    "        )),\n",
    "        ('gbm_conservative', GradientBoostingClassifier(\n",
    "            n_estimators=400, learning_rate=0.08, max_depth=3,\n",
    "            min_samples_split=30, min_samples_leaf=15,\n",
    "            random_state=random_state, subsample=0.8\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    # Meta-learner optimized for precision\n",
    "    meta_learner = LogisticRegression(\n",
    "        C=0.8, penalty='l2', class_weight=class_weights,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=3,\n",
    "        stack_method='predict_proba',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    stacking_model.fit(X_train_balanced, y_train_balanced)\n",
    "    stack_val_proba = stacking_model.predict_proba(X_val_eng)[:, 1]\n",
    "    \n",
    "    stack_threshold = 0.42  # Conservative threshold\n",
    "    stack_val_pred = (stack_val_proba >= stack_threshold).astype(int)\n",
    "    \n",
    "    models['stacking_ensemble'] = {\n",
    "        'model': stacking_model,\n",
    "        'scaler': None,\n",
    "        'val_predictions': stack_val_pred,\n",
    "        'val_probabilities': stack_val_proba,\n",
    "        'threshold': stack_threshold,\n",
    "        'features': engineered_features,\n",
    "        'requires_scaling': False\n",
    "    }\n",
    "    \n",
    "    stack_metrics = {\n",
    "        'recall': recall_score(y_val, stack_val_pred),\n",
    "        'precision': precision_score(y_val, stack_val_pred),\n",
    "        'f1': f1_score(y_val, stack_val_pred)\n",
    "    }\n",
    "    print(f\"Stacking Ensemble - Recall: {stack_metrics['recall']:.4f}, Precision: {stack_metrics['precision']:.4f}, F1: {stack_metrics['f1']:.4f}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def tiered_threshold_optimization(models, data_splits, target_recall=0.90):\n",
    "    \"\"\"\n",
    "    Implement tiered intervention strategy with multiple thresholds\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TIERED INTERVENTION THRESHOLD OPTIMIZATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    y_val = data_splits['y_val']\n",
    "    tiered_results = []\n",
    "    \n",
    "    for model_name, model_info in models.items():\n",
    "        y_proba = model_info['val_probabilities']\n",
    "        \n",
    "        print(f\"\\nOptimizing {model_name.replace('_', ' ').title()}...\")\n",
    "        \n",
    "        # Define risk tiers\n",
    "        high_risk_threshold = 0.70   # High precision tier\n",
    "        medium_risk_threshold = 0.40  # Balanced tier\n",
    "        low_risk_threshold = 0.20    # High recall tier\n",
    "        \n",
    "        # High risk predictions (highest precision)\n",
    "        high_risk_pred = (y_proba >= high_risk_threshold).astype(int)\n",
    "        \n",
    "        # Medium risk predictions  \n",
    "        medium_risk_pred = (y_proba >= medium_risk_threshold).astype(int)\n",
    "        \n",
    "        # Low risk predictions (highest recall)\n",
    "        low_risk_pred = (y_proba >= low_risk_threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics for each tier\n",
    "        tiers = {\n",
    "            'high_risk': {\n",
    "                'threshold': high_risk_threshold,\n",
    "                'predictions': high_risk_pred,\n",
    "                'recall': recall_score(y_val, high_risk_pred),\n",
    "                'precision': precision_score(y_val, high_risk_pred) if high_risk_pred.sum() > 0 else 0.0,\n",
    "                'f1': f1_score(y_val, high_risk_pred) if high_risk_pred.sum() > 0 else 0.0,\n",
    "                'coverage': high_risk_pred.sum() / len(y_val)\n",
    "            },\n",
    "            'medium_risk': {\n",
    "                'threshold': medium_risk_threshold,\n",
    "                'predictions': medium_risk_pred,\n",
    "                'recall': recall_score(y_val, medium_risk_pred),\n",
    "                'precision': precision_score(y_val, medium_risk_pred),\n",
    "                'f1': f1_score(y_val, medium_risk_pred),\n",
    "                'coverage': medium_risk_pred.sum() / len(y_val)\n",
    "            },\n",
    "            'low_risk': {\n",
    "                'threshold': low_risk_threshold,\n",
    "                'predictions': low_risk_pred,\n",
    "                'recall': recall_score(y_val, low_risk_pred),\n",
    "                'precision': precision_score(y_val, low_risk_pred),\n",
    "                'f1': f1_score(y_val, low_risk_pred),\n",
    "                'coverage': low_risk_pred.sum() / len(y_val)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Store tiered results\n",
    "        for tier_name, tier_metrics in tiers.items():\n",
    "            tiered_results.append({\n",
    "                'Model': model_name.replace('_', ' ').title(),\n",
    "                'Tier': tier_name.replace('_', ' ').title(),\n",
    "                'Threshold': tier_metrics['threshold'],\n",
    "                'Recall': tier_metrics['recall'],\n",
    "                'Precision': tier_metrics['precision'],\n",
    "                'F1': tier_metrics['f1'],\n",
    "                'Coverage': tier_metrics['coverage'],\n",
    "                'AUC': roc_auc_score(y_val, y_proba)\n",
    "            })\n",
    "        \n",
    "        # Update model with tiered predictions\n",
    "        models[model_name]['tiered_predictions'] = tiers\n",
    "        \n",
    "        # Display tier results\n",
    "        print(f\"  High Risk Tier (≥{high_risk_threshold}): Precision={tiers['high_risk']['precision']:.4f}, Recall={tiers['high_risk']['recall']:.4f}, Coverage={tiers['high_risk']['coverage']:.2%}\")\n",
    "        print(f\"  Medium Risk Tier (≥{medium_risk_threshold}): Precision={tiers['medium_risk']['precision']:.4f}, Recall={tiers['medium_risk']['recall']:.4f}, Coverage={tiers['medium_risk']['coverage']:.2%}\")\n",
    "        print(f\"  Low Risk Tier (≥{low_risk_threshold}): Precision={tiers['low_risk']['precision']:.4f}, Recall={tiers['low_risk']['recall']:.4f}, Coverage={tiers['low_risk']['coverage']:.2%}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(tiered_results)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TIERED INTERVENTION RESULTS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(results_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Analysis\n",
    "    high_precision_tiers = results_df[(results_df['Tier'] == 'High Risk') & (results_df['Precision'] >= 0.70)]\n",
    "    high_recall_tiers = results_df[(results_df['Tier'] == 'Low Risk') & (results_df['Recall'] >= 0.90)]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TIERED STRATEGY SUMMARY:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"High-precision tiers (≥70% precision): {len(high_precision_tiers)}\")\n",
    "    print(f\"High-recall tiers (≥90% recall): {len(high_recall_tiers)}\")\n",
    "    \n",
    "    if len(high_precision_tiers) > 0:\n",
    "        best_precision_tier = high_precision_tiers.loc[high_precision_tiers['Precision'].idxmax()]\n",
    "        print(f\"\\nBest high-precision tier: {best_precision_tier['Model']} - {best_precision_tier['Tier']}\")\n",
    "        print(f\"  Precision: {best_precision_tier['Precision']:.4f}\")\n",
    "        print(f\"  Recall: {best_precision_tier['Recall']:.4f}\")\n",
    "        print(f\"  Population Coverage: {best_precision_tier['Coverage']:.2%}\")\n",
    "    \n",
    "    if len(high_recall_tiers) > 0:\n",
    "        best_recall_tier = high_recall_tiers.loc[high_recall_tiers['Recall'].idxmax()]\n",
    "        print(f\"\\nBest high-recall tier: {best_recall_tier['Model']} - {best_recall_tier['Tier']}\")\n",
    "        print(f\"  Recall: {best_recall_tier['Recall']:.4f}\")\n",
    "        print(f\"  Precision: {best_recall_tier['Precision']:.4f}\")\n",
    "        print(f\"  Population Coverage: {best_recall_tier['Coverage']:.2%}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def comprehensive_evaluation(models, data_splits, tiered_results):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with focus on achievable targets\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE EVALUATION - REALISTIC TARGETS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    y_val = data_splits['y_val']\n",
    "    \n",
    "    # Standard evaluation\n",
    "    standard_results = []\n",
    "    for model_name, model_info in models.items():\n",
    "        y_pred = model_info['val_predictions']\n",
    "        y_proba = model_info['val_probabilities']\n",
    "        \n",
    "        standard_results.append({\n",
    "            'Model': model_name.replace('_', ' ').title(),\n",
    "            'AUC': roc_auc_score(y_val, y_proba),\n",
    "            'Precision': precision_score(y_val, y_pred),\n",
    "            'Recall': recall_score(y_val, y_pred),\n",
    "            'F1': f1_score(y_val, y_pred),\n",
    "            'Accuracy': accuracy_score(y_val, y_pred),\n",
    "            'Threshold': model_info['threshold']\n",
    "        })\n",
    "    \n",
    "    standard_df = pd.DataFrame(standard_results)\n",
    "    \n",
    "    print(\"STANDARD MODEL PERFORMANCE:\")\n",
    "    print(\"=\" * 85)\n",
    "    print(standard_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Best achievable combinations\n",
    "    best_precision_model = standard_df.loc[standard_df['Precision'].idxmax()]\n",
    "    best_recall_model = standard_df.loc[standard_df['Recall'].idxmax()]\n",
    "    best_f1_model = standard_df.loc[standard_df['F1'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nBEST INDIVIDUAL PERFORMANCES:\")\n",
    "    print(f\"Highest Precision: {best_precision_model['Model']} - {best_precision_model['Precision']:.4f}\")\n",
    "    print(f\"Highest Recall: {best_recall_model['Model']} - {best_recall_model['Recall']:.4f}\")\n",
    "    print(f\"Highest F1: {best_f1_model['Model']} - {best_f1_model['F1']:.4f}\")\n",
    "    \n",
    "    # Realistic targets analysis\n",
    "    high_recall_models = standard_df[standard_df['Recall'] >= 0.85]\n",
    "    decent_precision_models = standard_df[standard_df['Precision'] >= 0.65]\n",
    "    balanced_models = standard_df[(standard_df['Recall'] >= 0.85) & (standard_df['Precision'] >= 0.65)]\n",
    "    \n",
    "    print(f\"\\nREALISTIC PERFORMANCE ANALYSIS:\")\n",
    "    print(f\"Models with ≥85% recall: {len(high_recall_models)}/{len(standard_df)}\")\n",
    "    print(f\"Models with ≥65% precision: {len(decent_precision_models)}/{len(standard_df)}\")\n",
    "    print(f\"Models achieving BOTH (≥85% recall & ≥65% precision): {len(balanced_models)}/{len(standard_df)}\")\n",
    "    \n",
    "    if len(balanced_models) > 0:\n",
    "        print(f\"\\nACHIEVABLE BALANCED MODELS:\")\n",
    "        for _, model in balanced_models.iterrows():\n",
    "            print(f\"  {model['Model']}: Recall={model['Recall']:.4f}, Precision={model['Precision']:.4f}, F1={model['F1']:.4f}\")\n",
    "    \n",
    "    return standard_df\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Execute the comprehensive precision-recall optimization pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_path = r\"C:\\Users\\USER\\Desktop\\MUKABUGINGO_THESIS_CODES\\ANALYSIS\\rwanda_dhs_processed.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        print(f\"Dataset loaded successfully: {df.shape}\")\n",
    "        \n",
    "        target_col = 'early_sexual_debut'\n",
    "        if target_col not in df.columns:\n",
    "            raise ValueError(f\"Target variable '{target_col}' not found in dataset!\")\n",
    "        \n",
    "        # Phase 3.1: Enhanced data preparation\n",
    "        print(\"\\nStarting Phase 3.1: Enhanced Data Preparation...\")\n",
    "        data_splits = prepare_enhanced_data(df, target_col=target_col)\n",
    "        \n",
    "        # Phase 3.2: Precision-optimized model implementation\n",
    "        print(\"\\nStarting Phase 3.2: Precision-Optimized Model Implementation...\")\n",
    "        models = implement_precision_optimized_models(data_splits)\n",
    "        \n",
    "        # Phase 3.3: Tiered threshold optimization\n",
    "        print(\"\\nStarting Phase 3.3: Tiered Intervention Strategy...\")\n",
    "        tiered_results = tiered_threshold_optimization(models, data_splits, target_recall=0.90)\n",
    "        \n",
    "        # Phase 3.4: Comprehensive evaluation\n",
    "        print(\"\\nStarting Phase 3.4: Comprehensive Evaluation...\")\n",
    "        standard_results = comprehensive_evaluation(models, data_splits, tiered_results)\n",
    "        \n",
    "        # FINAL SUMMARY\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PRECISION-RECALL OPTIMIZATION COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Find best achievable performance\n",
    "        best_balanced = standard_results.loc[\n",
    "            (standard_results['Recall'] >= 0.85) & \n",
    "            (standard_results['Precision'] >= 0.65)\n",
    "        ]\n",
    "        \n",
    "        if len(best_balanced) > 0:\n",
    "            optimal_model = best_balanced.loc[best_balanced['F1'].idxmax()]\n",
    "            print(f\"\\nOPTIMAL ACHIEVABLE PERFORMANCE:\")\n",
    "            print(f\"Model: {optimal_model['Model']}\")\n",
    "            print(f\"  Recall: {optimal_model['Recall']:.4f} ({optimal_model['Recall']*100:.1f}%)\")\n",
    "            print(f\"  Precision: {optimal_model['Precision']:.4f} ({optimal_model['Precision']*100:.1f}%)\")\n",
    "            print(f\"  F1 Score: {optimal_model['F1']:.4f}\")\n",
    "            print(f\"  AUC: {optimal_model['AUC']:.4f}\")\n",
    "            print(f\"  Cases missed: {(1-optimal_model['Recall'])*100:.1f}%\")\n",
    "            print(f\"  False positive rate: {(1-optimal_model['Precision'])*100:.1f}%\")\n",
    "        else:\n",
    "            print(f\"\\nRECOMMENDED APPROACH: Use tiered intervention strategy\")\n",
    "            print(\"Different thresholds for different intervention intensities\")\n",
    "        \n",
    "        # Tiered strategy recommendations\n",
    "        high_precision_tiers = tiered_results[(tiered_results['Tier'] == 'High Risk') & \n",
    "                                            (tiered_results['Precision'] >= 0.70)]\n",
    "        \n",
    "        if len(high_precision_tiers) > 0:\n",
    "            best_precision_tier = high_precision_tiers.loc[high_precision_tiers['Precision'].idxmax()]\n",
    "            print(f\"\\nTIERED STRATEGY RECOMMENDATION:\")\n",
    "            print(f\"High-Precision Tier: {best_precision_tier['Model']} at threshold {best_precision_tier['Threshold']}\")\n",
    "            print(f\"  Precision: {best_precision_tier['Precision']:.4f}\")\n",
    "            print(f\"  Recall: {best_precision_tier['Recall']:.4f}\")\n",
    "            print(f\"  Population Coverage: {best_precision_tier['Coverage']:.2%}\")\n",
    "            print(f\"  Intervention: Intensive individual support\")\n",
    "        \n",
    "        print(f\"\\nREALISTIC TARGET ACHIEVEMENT:\")\n",
    "        recall_85_count = len(standard_results[standard_results['Recall'] >= 0.85])\n",
    "        precision_68_count = len(standard_results[standard_results['Precision'] >= 0.68])\n",
    "        print(f\"  Models achieving ≥85% recall: {recall_85_count}/{len(standard_results)}\")\n",
    "        print(f\"  Models achieving ≥68% precision: {precision_68_count}/{len(standard_results)}\")\n",
    "        \n",
    "        print(f\"\\nDEPLOYMENT READINESS:\")\n",
    "        print(\"  Advanced feature engineering completed\")\n",
    "        print(\"  Conservative sampling strategy applied\") \n",
    "        print(\"  Tiered intervention thresholds optimized\")\n",
    "        print(\"  Realistic performance targets identified\")\n",
    "        print(\"  Ready for pilot implementation\")\n",
    "        \n",
    "        return data_splits, models, standard_results, tiered_results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: Dataset not found at specified path\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in precision-recall optimization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    data_splits, models, standard_results, tiered_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54dd122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
